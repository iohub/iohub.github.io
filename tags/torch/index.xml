<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>torch on LifeIO</title>
    <link>https://iohub.github.io/tags/torch/</link>
    <description>Recent content in torch on LifeIO</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Jun 2024 12:00:00 +0000</lastBuildDate>
    <atom:link href="https://iohub.github.io/tags/torch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>transformers恢复训练填坑</title>
      <link>https://iohub.github.io/2024/06/06/resume-train/</link>
      <pubDate>Thu, 06 Jun 2024 12:00:00 +0000</pubDate>
      <guid>https://iohub.github.io/2024/06/06/resume-train/</guid>
      <description>0. 问题现象根据官方文档描述，设置resume_from_checkpoint参数为待恢复的检查点。 trainer.train(resume_from_checkpoint=&amp;#39;finetuned/checkpoint-5000&amp;#39;) 恢复训练后发现train loss、eva</description>
    </item>
    <item>
      <title>pytorch多机分布式训练</title>
      <link>https://iohub.github.io/2024/06/05/torch-nccl/</link>
      <pubDate>Wed, 05 Jun 2024 12:00:00 +0000</pubDate>
      <guid>https://iohub.github.io/2024/06/05/torch-nccl/</guid>
      <description>0. 环境准备pip install transformers datasets peft tensorboard # 安装flash-attation pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl 1. Master节点# 配置RDMA网卡 epoxrt NCCL_SOCKET_FAMILY=AF_INET epoxrt NCCL_SOCKET_IFNAME=eno3p1 export NCCL_IB_HCA=mlx5_1 # 配置nccl日志 export NCCL_DEBUG=INFO # 在</description>
    </item>
  </channel>
</rss>
